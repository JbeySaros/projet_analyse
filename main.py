"""
Point d'entr√©e principal de l'application d'analyse de donn√©es.
Orchestre tous les modules pour un workflow complet.
"""
import sys
from pathlib import Path
from typing import Optional
import argparse
import pandas as pd

from config import settings
from utils.logger import get_logger, PerformanceLogger
from data_loader.csv_loader import DataLoaderRepository
from data_loader.data_validator import DataValidator, ValidationSchema
from data_processor.cleaner import DataCleaner, ImputationStrategy
from data_processor.aggregator import DataAggregator
from data_processor.statistics import StatisticsCalculator
from visualization.chart_builder import ChartBuilder
from visualization.report_generator import ReportGenerator


logger = get_logger(__name__)


class DataAnalysisPipeline:
    """
    Pipeline complet d'analyse de donn√©es.
    
    Workflow:
    1. Chargement des donn√©es
    2. Validation
    3. Nettoyage
    4. Analyse et statistiques
    5. Visualisations
    6. G√©n√©ration de rapport
    7. Export des r√©sultats
    
    Example:
        >>> pipeline = DataAnalysisPipeline()
        >>> pipeline.run("data/vente_2025.csv", output_dir="output/")
    """
    
    def __init__(self):
        """Initialise tous les composants du pipeline."""
        logger.info("=" * 80)
        logger.info("Initialisation du Pipeline d'Analyse de Donn√©es")
        logger.info("=" * 80)
        
        self.loader = DataLoaderRepository()
        self.validator = DataValidator(strict_mode=False)
        self.cleaner = DataCleaner()
        self.aggregator = DataAggregator()
        self.stats_calc = StatisticsCalculator()
        self.chart_builder = ChartBuilder()
        self.report_gen = ReportGenerator()
        
        self.df = None
        self.df_clean = None
        
        logger.info("Pipeline initialis√© avec succ√®s")
    
    def run(
        self,
        file_path: str,
        output_dir: str = "outputs",
        skip_cleaning: bool = False,
        skip_validation: bool = False,
        generate_report: bool = True,
        export_excel: bool = True
    ) -> bool:
        """
        Execute le pipeline complet d'analyse.
        
        Args:
            file_path: Chemin du fichier de donn√©es
            output_dir: R√©pertoire de sortie
            skip_cleaning: Ignorer l'√©tape de nettoyage
            skip_validation: Ignorer la validation
            generate_report: G√©n√©rer le rapport HTML/PDF
            export_excel: Exporter les r√©sultats en Excel
            
        Returns:
            bool: True si succ√®s, False sinon
        """
        with PerformanceLogger(logger, "PIPELINE COMPLET"):
            try:
                logger.info("\n" + "=" * 80)
                logger.info("D√âMARRAGE DU PIPELINE D'ANALYSE")
                logger.info("=" * 80)
                logger.info(f"Fichier source: {file_path}")
                logger.info(f"R√©pertoire de sortie: {output_dir}")
                
                output_path = Path(output_dir)
                output_path.mkdir(parents=True, exist_ok=True)
                
                # √âtape 1: Chargement
                if not self.load_data(file_path):
                    return False
                
                # √âtape 2: Validation
                if not skip_validation:
                    if not self.validate_data():
                        logger.warning("Validation √©chou√©e, mais continuation du pipeline")
                
                # √âtape 3: Nettoyage
                if not skip_cleaning:
                    self.clean_data()
                else:
                    self.df_clean = self.df.copy()
                    logger.info("Nettoyage ignor√© (skip_cleaning=True)")
                
                # √âtape 4: Analyse et statistiques
                kpis = self.calculate_kpis()
                self.display_kpis(kpis)
                
                # √âtape 5: Agr√©gations
                aggregations = self.perform_aggregations()
                
                # √âtape 6: Statistiques avanc√©es
                stats_report = self.generate_statistics()
                
                # √âtape 7: Visualisations
                charts_paths = self.generate_visualizations(output_path)
                
                # √âtape 8: Rapport HTML/PDF
                if generate_report:
                    report_path = self.generate_report(output_path)
                    logger.info(f"‚úì Rapport g√©n√©r√©: {report_path}")
                
                # √âtape 9: Export Excel
                if export_excel:
                    excel_path = self.export_results_excel(
                        output_path,
                        aggregations,
                        stats_report
                    )
                    logger.info(f"‚úì Export Excel: {excel_path}")
                
                logger.info("\n" + "=" * 80)
                logger.info("‚úì PIPELINE TERMIN√â AVEC SUCC√àS")
                logger.info("=" * 80)
                
                return True
            
            except Exception as e:
                logger.error(f"‚úó √âCHEC DU PIPELINE: {str(e)}", exc_info=True)
                return False
    
    def load_data(self, file_path: str) -> bool:
        """
        Charge les donn√©es depuis un fichier.
        
        Args:
            file_path: Chemin du fichier
            
        Returns:
            bool: True si succ√®s
        """
        logger.info("\n[√âTAPE 1/9] Chargement des donn√©es")
        logger.info("-" * 80)
        
        try:
            self.df = self.loader.load_data(file_path)
            
            logger.info(f"‚úì Donn√©es charg√©es: {len(self.df)} lignes, {len(self.df.columns)} colonnes")
            logger.info(f"  Colonnes: {list(self.df.columns)}")
            logger.info(f"  M√©moire utilis√©e: {self.df.memory_usage(deep=True).sum() / (1024**2):.2f} MB")
            
            return True
        except Exception as e:
            logger.error(f"‚úó √âchec du chargement: {str(e)}")
            return False
    
    def validate_data(self) -> bool:
        """
        Valide la qualit√© des donn√©es.
        
        Returns:
            bool: True si validation r√©ussie
        """
        logger.info("\n[√âTAPE 2/9] Validation des donn√©es")
        logger.info("-" * 80)
        
        # Utiliser le sch√©ma de validation pour les ventes
        result = self.validator.validate_sales_data(self.df)
        
        logger.info(f"R√©sultat de validation: {'‚úì VALIDE' if result.is_valid else '‚úó INVALIDE'}")
        
        if result.errors:
            logger.error(f"Erreurs ({len(result.errors)}):")
            for error in result.errors:
                logger.error(f"  - {error}")
        
        if result.warnings:
            logger.warning(f"Avertissements ({len(result.warnings)}):")
            for warning in result.warnings:
                logger.warning(f"  - {warning}")
        
        # Afficher les m√©triques de qualit√©
        logger.info("M√©triques de qualit√©:")
        for key, value in result.metrics.items():
            if key != 'column_types':
                logger.info(f"  {key}: {value}")
        
        return result.is_valid
    
    def clean_data(self):
        """Nettoie les donn√©es."""
        logger.info("\n[√âTAPE 3/9] Nettoyage des donn√©es")
        logger.info("-" * 80)
        
        # Conversion de la date
        if 'date' in self.df.columns:
            self.df = self.cleaner.convert_dates(self.df, ['date'])
        
        # Nettoyage complet
        self.df_clean = self.cleaner.clean(
            self.df,
            remove_outliers=False,  # Configurable
            impute_missing=True,
            normalize=False,
            encode_categorical=False,
            clean_strings=True
        )
        
        logger.info(f"‚úì Nettoyage termin√©")
        logger.info(f"  Lignes avant: {len(self.df)}, apr√®s: {len(self.df_clean)}")
    
    def calculate_kpis(self) -> dict:
        """
        Calcule les KPIs m√©tier.
        
        Returns:
            dict: KPIs calcul√©s
        """
        logger.info("\n[√âTAPE 4/9] Calcul des KPIs")
        logger.info("-" * 80)
        
        kpis = self.aggregator.calculate_kpis(self.df_clean)
        
        logger.info("‚úì KPIs calcul√©s")
        return kpis
    
    def display_kpis(self, kpis: dict):
        """Affiche les KPIs de mani√®re format√©e."""
        logger.info("\nüìä INDICATEURS CL√âS DE PERFORMANCE (KPIs)")
        logger.info("-" * 80)
        logger.info(f"üí∞ Chiffre d'Affaires Total:  {kpis.get('revenue_total', 0):>15,.2f} ‚Ç¨")
        logger.info(f"üì¶ Transactions:              {kpis.get('transaction_count', 0):>15,}")
        logger.info(f"üõí Panier Moyen:              {kpis.get('average_basket', 0):>15,.2f} ‚Ç¨")
        logger.info(f"üìä Quantit√© Totale:           {kpis.get('total_quantity', 0):>15,}")
        logger.info(f"üíµ Prix Moyen:                {kpis.get('average_price', 0):>15,.2f} ‚Ç¨")
        logger.info(f"üè∑Ô∏è  Produits Uniques:          {kpis.get('unique_products', 0):>15,}")
        logger.info(f"üìÅ Cat√©gories:                {kpis.get('unique_categories', 0):>15,}")
        logger.info(f"üó∫Ô∏è  Villes:                    {kpis.get('unique_cities', 0):>15,}")
        logger.info("-" * 80)
    
    def perform_aggregations(self) -> dict:
        """
        Effectue les agr√©gations principales.
        
        Returns:
            dict: R√©sultats des agr√©gations
        """
        logger.info("\n[√âTAPE 5/9] Agr√©gations des donn√©es")
        logger.info("-" * 80)
        
        results = {}
        
        # Ventes par cat√©gorie
        results['by_category'] = self.aggregator.calculate_sales_by_category(self.df_clean)
        logger.info(f"‚úì Ventes par cat√©gorie: {len(results['by_category'])} cat√©gories")
        
        # Ventes par ville
        results['by_city'] = self.aggregator.calculate_sales_by_city(self.df_clean)
        logger.info(f"‚úì Ventes par ville: {len(results['by_city'])} villes")
        
        # Ventes par source
        if 'source' in self.df_clean.columns:
            results['by_source'] = self.aggregator.calculate_sales_by_source(self.df_clean)
            logger.info(f"‚úì Ventes par source: {len(results['by_source'])} sources")
        
        # Top produits
        results['top_products'] = self.aggregator.calculate_top_products(self.df_clean, top_n=10)
        logger.info(f"‚úì Top 10 produits identifi√©s")
        
        # Analyse temporelle
        if 'date' in self.df_clean.columns:
            results['trend'] = self.aggregator.calculate_trend_analysis(
                self.df_clean,
                'date',
                period='M'
            )
            logger.info(f"‚úì Analyse de tendance mensuelle: {len(results['trend'])} p√©riodes")
        
        return results
    
    def generate_statistics(self) -> dict:
        """
        G√©n√®re les statistiques avanc√©es.
        
        Returns:
            dict: Rapport statistique
        """
        logger.info("\n[√âTAPE 6/9] Calcul des statistiques")
        logger.info("-" * 80)
        
        stats_report = self.stats_calc.generate_statistics_report(self.df_clean)
        
        logger.info("‚úì Statistiques g√©n√©r√©es:")
        logger.info(f"  - {len(stats_report['descriptive_stats'])} variables analys√©es")
        logger.info(f"  - {len(stats_report['missing_values'])} variables avec valeurs manquantes")
        logger.info(f"  - {len(stats_report['outliers'])} variables avec outliers")
        
        return stats_report
    
    def generate_visualizations(self, output_path: Path) -> dict:
        """
        G√©n√®re les visualisations principales.
        
        Args:
            output_path: R√©pertoire de sortie
            
        Returns:
            dict: Chemins des graphiques g√©n√©r√©s
        """
        logger.info("\n[√âTAPE 7/9] G√©n√©ration des visualisations")
        logger.info("-" * 80)
        
        charts_dir = output_path / "charts"
        charts_dir.mkdir(exist_ok=True)
        
        paths = {}
        
        # 1. Ventes par cat√©gorie
        cat_sales = self.aggregator.calculate_sales_by_category(self.df_clean)
        fig1 = self.chart_builder.create_bar_chart(
            cat_sales,
            x='categorie',
            y='ca_total',
            title='Chiffre d\'Affaires par Cat√©gorie'
        )
        path1 = charts_dir / "ventes_categorie.html"
        self.chart_builder.save_chart(fig1, path1)
        paths['by_category'] = path1
        logger.info(f"‚úì Graphique 1/5: {path1.name}")
        
        # 2. R√©partition par ville (pie chart)
        city_sales = self.aggregator.calculate_sales_by_city(self.df_clean)
        fig2 = self.chart_builder.create_pie_chart(
            city_sales.head(10),
            names='ville',
            values='ca_total',
            title='R√©partition du CA par Ville (Top 10)',
            hole=0.3
        )
        path2 = charts_dir / "repartition_villes.html"
        self.chart_builder.save_chart(fig2, path2)
        paths['by_city'] = path2
        logger.info(f"‚úì Graphique 2/5: {path2.name}")
        
        # 3. √âvolution temporelle
        if 'date' in self.df_clean.columns:
            trend = self.aggregator.calculate_trend_analysis(self.df_clean, 'date', period='M')
            fig3 = self.chart_builder.create_line_chart(
                trend,
                x='date',
                y='ca_total',
                title='√âvolution Mensuelle du Chiffre d\'Affaires'
            )
            path3 = charts_dir / "evolution_ca.html"
            self.chart_builder.save_chart(fig3, path3)
            paths['trend'] = path3
            logger.info(f"‚úì Graphique 3/5: {path3.name}")
        
        # 4. Top produits
        top_products = self.aggregator.calculate_top_products(self.df_clean, top_n=10)
        fig4 = self.chart_builder.create_bar_chart(
            top_products,
            x='produit',
            y='ca_total',
            title='Top 10 Produits par CA',
            orientation='h'
        )
        path4 = charts_dir / "top_produits.html"
        self.chart_builder.save_chart(fig4, path4)
        paths['top_products'] = path4
        logger.info(f"‚úì Graphique 4/5: {path4.name}")
        
        # 5. Matrice de corr√©lation (si applicable)
        numeric_cols = self.df_clean.select_dtypes(include=['number']).columns
        if len(numeric_cols) > 1:
            corr_matrix = self.stats_calc.calculate_correlation_matrix(self.df_clean)
            fig5 = self.chart_builder.create_heatmap(
                corr_matrix,
                title='Matrice de Corr√©lation'
            )
            path5 = charts_dir / "correlation_matrix.html"
            self.chart_builder.save_chart(fig5, path5)
            paths['correlation'] = path5
            logger.info(f"‚úì Graphique 5/5: {path5.name}")
        
        logger.info(f"‚úì {len(paths)} graphiques g√©n√©r√©s dans {charts_dir}")
        return paths
    
    def generate_report(self, output_path: Path) -> Path:
        """
        G√©n√®re le rapport complet.
        
        Args:
            output_path: R√©pertoire de sortie
            
        Returns:
            Path: Chemin du rapport
        """
        logger.info("\n[√âTAPE 8/9] G√©n√©ration du rapport")
        logger.info("-" * 80)
        
        report_path = output_path / "rapport_analyse.html"
        
        result = self.report_gen.generate_sales_report(
            self.df_clean,
            output_path=str(report_path),
            format='html',
            include_charts=True
        )
        
        logger.info(f"‚úì Rapport HTML g√©n√©r√©: {result}")
        
        return result
    
    def export_results_excel(
        self,
        output_path: Path,
        aggregations: dict,
        stats_report: dict
    ) -> Path:
        """
        Exporte tous les r√©sultats en Excel.
        
        Args:
            output_path: R√©pertoire de sortie
            aggregations: R√©sultats des agr√©gations
            stats_report: Rapport statistique
            
        Returns:
            Path: Chemin du fichier Excel
        """
        logger.info("\n[√âTAPE 9/9] Export Excel")
        logger.info("-" * 80)
        
        excel_path = output_path / "resultats_analyse.xlsx"
        
        sheets = {
            'Donn√©es Nettoy√©es': self.df_clean.head(1000),  # Limiter √† 1000 lignes
            'Ventes par Cat√©gorie': aggregations['by_category'],
            'Ventes par Ville': aggregations['by_city'],
            'Top Produits': aggregations['top_products']
        }
        
        if 'trend' in aggregations:
            sheets['√âvolution Temporelle'] = aggregations['trend']
        
        result = self.report_gen.export_to_excel(
            self.df_clean.head(1000),
            output_path=str(excel_path),
            sheets=sheets
        )
        
        logger.info(f"‚úì Export Excel: {result}")
        
        return result


def main():
    """Fonction principale avec arguments en ligne de commande."""
    parser = argparse.ArgumentParser(
        description="Pipeline d'analyse de donn√©es de ventes"
    )
    parser.add_argument(
        'file',
        help="Chemin du fichier CSV √† analyser"
    )
    parser.add_argument(
        '-o', '--output',
        default='outputs',
        help="R√©pertoire de sortie (d√©faut: outputs)"
    )
    parser.add_argument(
        '--skip-cleaning',
        action='store_true',
        help="Ignorer l'√©tape de nettoyage"
    )
    parser.add_argument(
        '--skip-validation',
        action='store_true',
        help="Ignorer la validation"
    )
    parser.add_argument(
        '--no-report',
        action='store_true',
        help="Ne pas g√©n√©rer le rapport"
    )
    parser.add_argument(
        '--no-excel',
        action='store_true',
        help="Ne pas exporter en Excel"
    )
    
    args = parser.parse_args()
    
    # V√©rifier que le fichier existe
    if not Path(args.file).exists():
        logger.error(f"Fichier introuvable: {args.file}")
        sys.exit(1)
    
    # Ex√©cuter le pipeline
    pipeline = DataAnalysisPipeline()
    success = pipeline.run(
        file_path=args.file,
        output_dir=args.output,
        skip_cleaning=args.skip_cleaning,
        skip_validation=args.skip_validation,
        generate_report=not args.no_report,
        export_excel=not args.no_excel
    )
    
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
