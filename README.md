# üìä Data Analysis Platform - Plateforme d'Analyse de Donn√©es

[![Python Version](https://img.shields.io/badge/python-3.10%2B-blue)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104%2B-green)](https://fastapi.tiangolo.com/)
[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)

[![Jenkins](https://img.shields.io/badge/Jenkins-D24939?logo=jenkins&logoColor=white)](https://www.jenkins.io/)
[![Docker](https://img.shields.io/badge/Docker-2496ED?logo=docker&logoColor=white)](https://www.docker.com/)
[![Redis](https://img.shields.io/badge/Redis-DC382D?logo=redis&logoColor=white)](https://redis.io/)

> Plateforme SaaS compl√®te pour l'ingestion, le traitement et l'analyse de donn√©es de ventes multi-sources avec g√©n√©ration de rapports interactifs.

---

##  Vue d'Ensemble

Cette plateforme d'analyse de donn√©es est con√ßue comme un syst√®me **robuste**, **scalable** et **maintenable** permettant de :

-  Ing√©rer et traiter des volumes croissants de donn√©es CSV
-  Fournir une API REST pour acc√©der aux donn√©es et r√©sultats d'analyse
-  G√©n√©rer des rapports visuels interactifs (graphiques, dashboards)
-  G√©rer les erreurs et performances √† grande √©chelle
-  Documenter et tester le code pour une √©quipe de d√©veloppeurs

###  Architecture

```
projet_analyse/
‚îú‚îÄ‚îÄ data_loader/             # Couche d'entr√©e des donn√©es
‚îÇ   ‚îú‚îÄ‚îÄ csv_loader.py        # Chargement CSV avec Repository Pattern
‚îÇ   ‚îú‚îÄ‚îÄ data_validator.py    # Validation des donn√©es
‚îÇ   ‚îî‚îÄ‚îÄ exceptions.py        # Exceptions personnalis√©es
‚îÇ
‚îú‚îÄ‚îÄ data_processor/          # Couche de traitement
‚îÇ   ‚îú‚îÄ‚îÄ cleaner.py           # Nettoyage avanc√© (outliers, imputation)
‚îÇ   ‚îú‚îÄ‚îÄ aggregator.py        # Agr√©gations complexes et KPIs
‚îÇ   ‚îî‚îÄ‚îÄ statistics.py        # Statistiques descriptives et inf√©rentielles
‚îÇ
‚îú‚îÄ‚îÄ visualization/           # Couche de visualisation
‚îÇ   ‚îú‚îÄ‚îÄ chart_builder.py     # Cr√©ation de graphiques (Plotly/Matplotlib)
‚îÇ   ‚îî‚îÄ‚îÄ report_generator.py  # G√©n√©ration de rapports HTML/PDF
‚îÇ
‚îú‚îÄ‚îÄ api/                     # API REST FastAPI
‚îÇ   ‚îî‚îÄ‚îÄ main.py              # Endpoints et routes
‚îÇ
‚îú‚îÄ‚îÄ tests/                   # Tests unitaires et d'int√©gration
‚îÇ   ‚îî‚îÄ‚îÄ test_loader.py       # Exemple de tests avec pytest
‚îÇ
‚îú‚îÄ‚îÄ utils/                   # Utilitaires
‚îÇ   ‚îî‚îÄ‚îÄ logger.py            # Logging professionnel
‚îÇ
‚îú‚îÄ‚îÄ config.py                # Configuration centralis√©e
‚îú‚îÄ‚îÄ main.py                  # Point d'entr√©e du pipeline
‚îú‚îÄ‚îÄ requirements.txt         # D√©pendances Python
‚îî‚îÄ‚îÄ README.md                # Documentation
```

---

##  Installation

### Pr√©requis

- **Python 3.10+**
- **pip** ou **poetry**
- **Redis** (optionnel, pour le cache)
- **Docker** (optionnel, pour la conteneurisation)

### Installation Locale

1. **Cloner le repository**

```bash
git clone https://github.com/votre-username/data-analysis-platform.git
cd data-analysis-platform
```

2. **Cr√©er un environnement virtuel**

```bash
python -m venv venv

# Activation
# Windows
venv\Scripts\activate
# Linux/Mac
source venv/bin/activate
```

3. **Installer les d√©pendances**

```bash
pip install -r requirements.txt
```

4. **Configurer les variables d'environnement**

```bash
cp .env.example .env
# √âditer .env avec vos param√®tres
```

5. **Cr√©er les r√©pertoires n√©cessaires**

```bash
mkdir -p data uploads outputs logs
```

---

##  Utilisation

### 1. Pipeline en Ligne de Commande

Analyse compl√®te d'un fichier CSV :

```bash
python main.py data/vente_2025.csv -o outputs/
```

**Options disponibles :**

```bash
python main.py --help

Options:
  -o, --output DIR        R√©pertoire de sortie (d√©faut: outputs)
  --skip-cleaning         Ignorer l'√©tape de nettoyage
  --skip-validation       Ignorer la validation
  --no-report             Ne pas g√©n√©rer le rapport HTML/PDF
  --no-excel              Ne pas exporter en Excel
```

**Exemple complet :**

```bash
python main.py data/vente_2025.csv \
    --output resultats_janvier \
    --skip-validation
```

### 2. API REST

#### D√©marrage de l'API

```bash
# Mode d√©veloppement
python api/main.py

# Ou avec uvicorn
uvicorn api.main:app --reload --host 0.0.0.0 --port 8000
```

#### Documentation Interactive

Une fois l'API lanc√©e, acc√©dez √† :

- **Swagger UI** : http://localhost:8000/api/docs
- **ReDoc** : http://localhost:8000/api/redoc

#### Endpoints Principaux

#####  Upload de fichier

```bash
curl -X POST "http://localhost:8000/api/v1/upload" \
  -F "file=@data/vente_2025.csv" \
  -F "validate=true"
```

#####  Analyse compl√®te

```bash
curl -X POST "http://localhost:8000/api/v1/analyze" \
  -F "file=@data/vente_2025.csv" \
  -F "clean=true" \
  -F "remove_outliers=false"
```

#####  G√©n√©ration de graphique

```bash
curl -X POST "http://localhost:8000/api/v1/charts/bar" \
  -F "file=@data/vente_2025.csv" \
  -F "x=categorie" \
  -F "y=ca_total" \
  -F "title=Ventes par Cat√©gorie"
```

#####  G√©n√©ration de rapport

```bash
curl -X POST "http://localhost:8000/api/v1/reports/generate" \
  -F "file=@data/vente_2025.csv" \
  -F "format=html" \
  --output rapport.html
```

#####  Statistiques

```bash
# Statistiques descriptives
curl -X POST "http://localhost:8000/api/v1/stats/describe" \
  -F "file=@data/vente_2025.csv"

# Matrice de corr√©lation
curl -X POST "http://localhost:8000/api/v1/stats/correlation" \
  -F "file=@data/vente_2025.csv" \
  -F "method=pearson"
```

### 3. Utilisation Programmatique

```python
from main import DataAnalysisPipeline

# Cr√©er une instance du pipeline
pipeline = DataAnalysisPipeline()

# Ex√©cuter l'analyse compl√®te
success = pipeline.run(
    file_path="data/vente_2025.csv",
    output_dir="outputs/",
    skip_cleaning=False,
    generate_report=True,
    export_excel=True
)

if success:
    print("‚úì Analyse termin√©e avec succ√®s")
```

**Utilisation modulaire :**

```python
from data_loader.csv_loader import CSVLoader
from data_processor.aggregator import DataAggregator
from visualization.chart_builder import ChartBuilder

# Charger les donn√©es
loader = CSVLoader()
df = loader.load("data/vente_2025.csv")

# Calculer les KPIs
aggregator = DataAggregator()
kpis = aggregator.calculate_kpis(df)
print(f"CA Total: {kpis['revenue_total']:.2f}‚Ç¨")

# Cr√©er un graphique
builder = ChartBuilder()
fig = builder.create_bar_chart(df, x='categorie', y='prix')
builder.save_chart(fig, "output/chart.html")
```

---

##  Tests

### Ex√©cuter tous les tests

```bash
pytest
```

### Tests avec couverture

```bash
pytest --cov=data_loader --cov=data_processor --cov=visualization \
       --cov-report=html --cov-report=term-missing
```

### Tests par cat√©gorie

```bash
# Tests unitaires uniquement
pytest -m unit

# Tests d'int√©gration
pytest -m integration

# Tests d'un module sp√©cifique
pytest tests/test_loader.py -v
```

### Rapport de couverture

Apr√®s ex√©cution avec `--cov-report=html`, ouvrez :

```bash
open htmlcov/index.html  # Mac/Linux
start htmlcov/index.html # Windows
```

---

##  Fonctionnalit√©s D√©taill√©es

###  Chargement de Donn√©es

- **Formats support√©s** : CSV, Excel (XLSX, XLS)
- **D√©tection automatique** : Encodage et d√©limiteur
- **Gros fichiers** : Chargement par chunks
- **Validation** : Types, valeurs manquantes, doublons
- **Logging** : Tra√ßabilit√© compl√®te

```python
from data_loader.csv_loader import CSVLoader

loader = CSVLoader()
df = loader.load("data/vente_2025.csv")
# ‚úì Donn√©es charg√©es: 10,000 lignes, 7 colonnes
```

###  Nettoyage de Donn√©es

- **Outliers** : D√©tection IQR et Z-score
- **Imputation** : Mean, Median, KNN, Forward/Backward Fill
- **Normalisation** : Standard Scaler, MinMax, Robust
- **Encodage** : Label Encoding, One-Hot Encoding

```python
from data_processor.cleaner import DataCleaner

cleaner = DataCleaner()
df_clean = cleaner.clean(
    df,
    remove_outliers=True,
    impute_missing=True,
    normalize=False
)
```

###  Agr√©gations et KPIs

- **Groupby multi-niveaux**
- **Pivots et cross-tabs**
- **Time-series resampling**
- **KPIs m√©tier** : CA, panier moyen, top produits

```python
from data_processor.aggregator import DataAggregator

aggregator = DataAggregator()
kpis = aggregator.calculate_kpis(df)
sales_by_category = aggregator.calculate_sales_by_category(df)
```

###  Statistiques

- **Descriptives** : Mean, std, quartiles, skewness, kurtosis
- **Inf√©rentielles** : T-test, Chi2, intervalles de confiance
- **Corr√©lations** : Pearson, Spearman, Kendall
- **Tests de normalit√©** : Shapiro-Wilk, Kolmogorov-Smirnov

```python
from data_processor.statistics import StatisticsCalculator

stats = StatisticsCalculator()
summary = stats.describe_column(df, 'prix')
corr_matrix = stats.calculate_correlation_matrix(df)
```

###  Visualisations

**Types de graphiques** :
- Bar charts (vertical/horizontal)
- Line charts (courbes)
- Pie charts / Donuts
- Scatter plots (avec trendline)
- Heatmaps (corr√©lations)
- Histograms (distributions)
- Box plots

```python
from visualization.chart_builder import ChartBuilder

builder = ChartBuilder()

# Graphique en barres
fig = builder.create_bar_chart(df, x='categorie', y='ca_total')
builder.save_chart(fig, 'output/bar_chart.html')

# √âvolution temporelle
fig = builder.create_line_chart(df, x='date', y='ca')
```

###  G√©n√©ration de Rapports

- **Formats** : HTML (interactif), PDF
- **Contenu** : KPIs, tableaux, graphiques, statistiques
- **Templates** : Personnalisables avec Jinja2
- **Export** : Excel multi-feuilles

```python
from visualization.report_generator import ReportGenerator

generator = ReportGenerator()
report_path = generator.generate_sales_report(
    df,
    output_path='rapport.html',
    format='html',
    include_charts=True
)
```

---

## Patterns et Principes

### Design Patterns Impl√©ment√©s

1. **Repository Pattern** : Abstraction de l'acc√®s aux donn√©es
2. **Factory Pattern** : Cr√©ation d'objets (charts, loaders)
3. **Strategy Pattern** : Algorithmes interchangeables (cleaning)
4. **Singleton** : Logger, Configuration
5. **Dependency Injection** : FastAPI Depends()

### Principes SOLID

-  **Single Responsibility** : Chaque classe a un r√¥le unique
-  **Open/Closed** : Extensible sans modification
-  **Liskov Substitution** : Interfaces coh√©rentes
-  **Interface Segregation** : Interfaces sp√©cifiques
-  **Dependency Inversion** : D√©pendances abstraites

### Clean Code

-  **PEP 8** : Conformit√© au style Python
-  **Type Hints** : Typage statique avec mypy
-  **Docstrings** : Documentation compl√®te (Google style)
-  **DRY** : Don't Repeat Yourself
-  **KISS** : Keep It Stupid Simple

---

## üîß Configuration

Toute la configuration est centralis√©e dans `config.py` et peut √™tre surcharg√©e via variables d'environnement (`.env`).

### Param√®tres Principaux

```python
# Voir config.py pour la liste compl√®te

# Taille max des fichiers
MAX_FILE_SIZE = 100 * 1024 * 1024  # 100 MB

# M√©thode de d√©tection des outliers
OUTLIER_METHOD = "iqr"  # ou "zscore"

# Cache Redis
CACHE_TTL = 3600  # 1 heure
CACHE_ENABLED = True

# Logging
LOG_LEVEL = "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
```

---

##  Documentation API

### Mod√®les de Donn√©es

#### Upload Response

```json
{
  "success": true,
  "filename": "vente_2025.csv",
  "rows": 1000,
  "columns": 7,
  "column_names": ["date", "produit", "categorie", "prix", "quantite", "ville", "source"],
  "memory_mb": 0.5,
  "validation": {
    "is_valid": true,
    "errors": [],
    "warnings": []
  }
}
```

#### KPIs Response

```json
{
  "revenue_total": 125000.50,
  "transaction_count": 1000,
  "average_basket": 125.00,
  "total_quantity": 5000,
  "average_price": 25.00,
  "unique_products": 50,
  "unique_categories": 5,
  "unique_cities": 10
}
```

---

##  Docker

### Docker Compose

```yaml
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - REDIS_HOST=redis
    volumes:
      - ./data:/app/data
      - ./outputs:/app/outputs
    depends_on:
      - redis

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
```

### Commandes

```bash
# Build et d√©marrage
docker-compose up -d

# Logs
docker-compose logs -f api

# Arr√™t
docker-compose down
```

---

## üìù Licence

Ce projet est sous licence GNU GPL v3


